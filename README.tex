% !TeX program = pdfLaTeX
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{textcomp}
\usepackage[hyphens]{url} % not crucial - just used below for the URL
\usepackage{hyperref}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

%% load any required packages here


\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf How to share data for collaboration}

  \author{
        Shannon E. Ellis \thanks{We want to thank Jenny Bryan and the referees, Nicholas Horton and
Stephen Turner, for their helpful contributions and edits. We would
additionally like to thank Foram Ashar, Pat Carlson, Claude Chaunier,
Leonardo Collado-Torres, Dan Fowler, David Jankoski, Sean Kross, Gene
Miller, Leslie Myint, and Nick Reich for their suggestions and edits.} \\
    Department of Biostatistics, Johns Hopkins Bloomberg School of Public
    Health and Center for Computational Biology, Johns Hopkins University
    and Center for Computational Biology, Johns Hopkins University\\
     and \\     Jeffrey T. Leek \\
    Department of Biostatistics, Johns Hopkins Bloomberg School of Public
    Health and Center for Computational Biology, Johns Hopkins University
    and Center for Computational Biology, Johns Hopkins University\\
      }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf How to share data for collaboration}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Within the statistics community, a number of guiding principles for
sharing data have emerged; however, these principles are not always made
clear to collaborators generating the data. To bridge this divide, we
have established a set of guidelines for sharing data. In these, we
highlight the need to provide raw data to the statistician, the
importance of consistent formatting, and the necessity of including all
essential experimental information and pre-processing steps carried out
to the statistician. With these guidelines we hope to avoid errors and
delays in data analysis.
\end{abstract}

\noindent%
{\it Keywords:} tidy data, guidelines, statistician, data sharing, analysis
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{Introduction}\label{introduction}

A set of general principles for sharing data have emerged within the
statistics community
\citep[\citet{_tidy_data},\citet{wilson_good_2016},\citet{white_nine_2013}]{_data_org}.
But these principles are not always clear to researchers, scientists, or
collaborators generating the data. This has led to a disconnect between
those generating data and those analyzing it about the best way for data
to be shared. To bridge this divide, we have developed general
guidelines for anyone generating data who anticipates their data will be
shared with a statistician, data scientist, or analyst at some point
during their project. The goals of this guide are to provide some
instruction on the best way to share data to avoid the most common
pitfalls and sources of delay in the transition from data collection to
data analysis \citep{leek2015opinion}. This guide focuses on data
sharing between collaborators and does not directly address best
practices for how to make the data behind a published paper available in
public repositories. While we do not focus on the pitfalls and specific
requirements of that process here, following many of the guidelines we
present will be critical during that process as well.

When it comes to collaborations between data collectors and
statisticians, it is a reasonable expectation that the statistician
should be able to handle and analyze the data in whatever state they
arrive. For this to be possible, the statistician must be provided the
raw data, information on any steps taken to preprocess the data, and
enough information about the experimental conditions to allow the
statistician to identify and incorporate hidden sources of variability
into his or her analysis \citep{baggerly2010disclose}. On the data
generator's end, it can be expected that he or she will receive results
from a statistician in a reasonable amount of time. From our experience
in the Leek group \citep{_jtleek} (where we work with a large number of
collaborators to analyze data) and from conversations with other
statisticians, the number one source of delay in the speed of returning
results to collaborators is the condition of the data when they arrive.

To help meet the expectations of both the data generator and the
statistician, all of the necessary information must be provided and
provided in a consistent and well-organized manner to the statistician.
Consistent data sharing reduces the likelihood of errors during analysis
and also decreases analysis turnaround time.

We provide these guidelines on data sharing and explain the reasoning
behind them from the analyst's perspective. We envision these will be
useful to the following individuals:

\begin{itemize}
\tightlist
\item
  Collaborators who need statisticians or data scientists to analyze
  data
\item
  Students or postdocs in various disciplines looking for consulting
  advice
\item
  Junior statistics students whose job it is to collate, clean, and
  wrangle data sets
\item
  Statisticians or data scientists seeking a concise guide to share with
  collaborators to clarify best practices for data sharing
\end{itemize}

\section{What you should deliver to the
statistician}\label{what-you-should-deliver-to-the-statistician}

To facilitate the most efficient and timely analysis this is the
information you should pass to a statistician:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The raw data.
\item
  A tidy data set \citep{_tidy_data}.
\item
  A code book describing each variable and its values in the tidy data
  set.
\item
  An explicit and exact recipe you used to go from 1 -\textgreater{}
  2,3.
\end{enumerate}

For clarity, we will further define each part of the data package
transferred.

\subsection{The raw data}\label{the-raw-data}

It is critical that you include the rawest form of the data to which you
have access. This ensures that data provenance can be maintained
throughout the workflow. Here are some examples of the raw form of data:

\begin{itemize}
\tightlist
\item
  The strange binary file \citep{_binary_2017} your measurement machine
  spits out
\item
  The unformatted Excel file with 10 worksheets the company you
  contracted with sent you
\item
  The complicated JSON \citep{noauthor_json:_nodate} data you got from
  scraping the Twitter API \citep{noauthor_twitter_nodate}
\item
  The hand-entered numbers you collected looking through a microscope
\end{itemize}

You know the raw data are in the right format if you:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ran no software on the data
\item
  Did not modify any of the data values
\item
  Did not remove any data from the data set
\item
  Did not summarize the data in any way
\end{enumerate}

If you made any modifications to the raw data, it is not the raw form of
the data. For example, statisticians are often supplied summary
statistics (such as averages) rather than the underlying raw data used
to calculate these summary statistics. While the intent of the data
collector is to be helpful, the reality is that this slows the analyst
down. Statisticians can easily calculate any appropriate summary
statistics from the raw data. Being provided the raw data is essential
for accurate analysis.

It can often help to consider what would happen if new data for a study
were to arrive to the statistician. If this new data requires no
modification before being combined with the first set of raw data
provided, it is likely raw data. Reporting modified data as raw data is
a very common way to slow down the analysis process, since the analyst
will often have to do a forensic study of your data to figure out why
the raw data looks weird.

\subsection{The tidy data set}\label{the-tidy-data-set}

The general principles of tidy data have been laid out by Hadley Wickham
previously \citep{_tidy_data}. While the paper describes tidy data using
R \citep{_r}, the principles are more generally applicable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable you measure should be in one column.
\item
  Each different observation of that variable should be in a different
  row.
\item
  There should be one table for each ``kind'' of data.
\item
  If you have multiple tables, they should include a column in the table
  (with the same column label!) that allows them to be joined or merged
  (see \textbf{Figure \ref{fig1}A-B}).
\end{enumerate}

\begin{figure}[htbp]
\centering
\includegraphics{Fig1.pdf}
\caption{Data Organization \textbf{A}. Raw (and tidy) data. The raw
measurements taken during the experiment are included here for the 40
patients (rows) and 20 hormones (columns) measured. Indicating that
these are raw values conveys that no manipulation or computation has
been done on the included values. \textbf{B}. Tidy data. These data
convey the important clinical information to your statistician for each
sample (rows) across a number of variables (columns). With a single
value included in each cell and consistently and informatively named
column headers and values, these tidy data can be easily understood and
used by your statistician. Importantly, \texttt{PatientID} is coded in
the exact same way between Hormone\_Data\_raw.csv and
Demographic\_Data.csv enabling easy merging by the statistician.
\textbf{C}. Code Book. Here, all pertinent and detailed information
about both the experiment and the data are conveyed to the statistician.
This is the place for any extra detail that does not fit into the data
generator's tidy data tables or spreadsheets. \textbf{D}. Pseudocode.
This includes information about any processing steps taken to get the
data into the form in which the statistician is receiving it.
\label{fig1}}
\end{figure}

While these are the most critical decisions, there are a number of
additional things that will make your data set much easier to handle
\citep{_data_org} (summarized in \textbf{Box\ref{fig1}}). Briefly, it is
best to include a row at the top of each data table or spreadsheet that
contains informative column names. And, each cell should include only
one value or unit of information. Sentences should generally be avoided
here; any lengthy explanations should instead be included in the ``Code
book.''

To provide an experimental example, suppose you want to know if
individuals with diabetes have altered hormonal profiles. To answer this
question, you carry out an experiment in which blood is drawn from 20
individuals with diabetes as well as 20 healthy controls. This blood was
used to measure blood levels for 50 different hormones. These
measurements would comprise your first `kind' of data (see \#3 above).
You have also collected demographic information from the patients
including their age, sex, BMI, treatment, and diagnosis (your second
``kind'' of data). For this example, your first table or spreadsheet
would include the measurements for the 20 different hormones. This would
have 41 rows (a row for the name of the measured hormones at top and
then one row for each of the 40 individuals in your study) and 51
columns (one for \texttt{PatientID} and then one for each measurement)
(\textbf{Figure \ref{fig1}A}). You would have another table or
spreadsheet that contains the demographic information. It would have six
columns (\texttt{PatientID}, \texttt{Age}, \texttt{Sex}, \texttt{BMI},
\texttt{CollectionDate},\texttt{Diagnosis}) and 41 rows (a row with
variable names, then one row for each patient) (\textbf{Figure
\ref{fig1}B}).

With regards to the formatting of these data, if you are sharing your
data with the collaborator in Excel, the tidy data should be in one
Excel file per table. They should not have multiple worksheets, no
macros should be applied to the data, and no cells should be
highlighted. Alternatively, the data could be shared in either a CSV or
TAB-delimited text file. Caution should always be taken when reading CSV
files into Excel as it can sometimes lead to non-reproducible handling
of date variables, time variables, and variables that Excel incorrectly
assumes are date or time variables \citep{zeeberg_mistaken_2004}. For
example, Excel incorrectly assumes the gene \texttt{SEPT9} is the date
\texttt{Sept-9} due to default date format conversions. Floating-point
format conversions cause similar problems. To avoid these issues, use
ISO 8601 \citep{newman_date_nodate} guidelines when coding date and time
variables. (See \textbf{Box\ref{box1}}). Any time data is being
formatted, the tidier must be extremely careful that no unintended
alterations have been made. Spot checking data after tidying, which
includes, but is not limited to, ensuring the correct number of columns
and rows are present and that column labels are accurate and consistent
across spreadsheets, is crucial.

\begin{figure}[htbp]
\centering
\includegraphics{Box1.pdf}
\caption{Box1 \label{box1}}
\end{figure}

\subsection{The code book}\label{the-code-book}

For almost any data set, the measurements you calculate will need to be
described in more detail than you can or should sneak into the
spreadsheet. The code book (also referred to as a `data dictionary')
contains this information. At a minimum it should contain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Information about the variables (including units!) in the data set not
  contained in the tidy data.
\item
  Information about the summary choices you made.
\item
  Information about the experimental study design you used.
\end{enumerate}

In our blood example, the statistician would want to know what the unit
of measurement for each demographic variable is (age in years, treatment
dose, level of diagnosis and how heterogeneous). They would also want to
know any other information about how you did the data collection and
study design. For example, are these the first 20 patients that walked
into the clinic? Are they 20 highly selected patients by some
characteristic like age? Are they randomized to treatments? This is the
place for any detail about either the experimental design or the data
itself that may be informative to the statistician.

A common format for this document is a Word file. There should be a
section called ``Study design'' that has a thorough description of the
question being asked by the study as well as how you collected the data.
An additional section called ``Code book'' should be provided to
describe each variable and its units. This information is frequently
conveyed most simply in tabular form. In this case, the columns of the
table would contain columns including \texttt{VariableName},
\texttt{Description}, \texttt{Units}, \texttt{CodingNotes}, and
\texttt{OtherNotes}. Further columns that provide additional information
to the statistician should be included. (\textbf{Figure \ref{fig1}C})

\subsubsection{How to code variables}\label{how-to-code-variables}

When you put variables into a spreadsheet there are several main
categories you will run into depending on their data type:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Continuous
\item
  Ordinal
\item
  Categorical
\item
  Missing
\item
  Censored
\end{enumerate}

Continuous variables are anything measured on a quantitative scale that
could be any fractional number. An example would be something like
weight measured in kg. Ordinal data are data that have a fixed, small
(\textless{} 100) number of levels but are ordered. This could be for
example survey responses where the choices are: poor, fair, good.
Categorical data are data where there are multiple categories, but they
aren't ordered. One example would be sex: male or female. Missing data
are data that are unobserved and you don't know the mechanism. Missing
values should be coded as \texttt{NA}. If, however, missingness is coded
in an alternative manner, this should be explicitly noted in the code
book. Censored data are data where you know the missingness mechanism on
some level. Common examples are a measurement being below a detection
limit or a patient being lost to follow-up. They should also be coded as
\texttt{NA} when you don't have the data. But you should also add a new
column to your tidy data called, ``VariableNameCensored'' which should
have values of \texttt{TRUE} if censored and \texttt{FALSE} if not. In
the code book you should explain why those values are missing. It is
absolutely critical to report to the analyst if there is a reason you
know about that some of the data are missing. You should also not
impute, make up, or throw away missing observations.

Explanations for the reasoning behind variable coding guidelines can be
found in \textbf{Box\ref{box1}}. Generally, try to avoid coding
categorical or ordinal variables as numbers. When you enter the value
for sex in the tidy data, it should be ``male'' or ``female''. The
ordinal values in the data set should be ``poor'', ``fair'', and
``good'' not 1, 2, 3. This coding is attractive because it is
self-documenting; any ambiguity or need for interpretation by the
analyst is removed. This will ultimately avoid potential mix-ups about
which direction effects go and will help identify coding errors.

Always encode every piece of information about your observations using
text. For example, if you are storing data in Excel and use a form of
colored text or cell background formatting to indicate information about
an observation (``red variable entries were observed in experiment 1.'')
then this information will not be exported (and will be lost!) when the
data is exported as raw text. Every piece of data should be encoded as
actual text that can be exported.

\subsection{The instruction list}\label{the-instruction-list}

You may have heard this before, but reproducibility is a big deal in
computational science \citep{peng_reproducible_2011}. To accomplish this
goal, best practices have been discussed in detail previously
\citep{wilson_good_2016}. However, for simplicity here, this means that
when you submit your paper, the reviewers and the rest of the world
should be able to exactly replicate the analyses from raw data all the
way to final results. If you are trying to be efficient, you will likely
perform some summarization or data analysis steps before the data can be
considered tidy and passed off to your statistician.

The ideal thing for you to do when performing summarization is to create
a computer script (in \texttt{R}, \texttt{Python}, or something else)
that takes the raw data as input and produces the tidy data you are
sharing as output. Ideally, this script would be run a couple of times
to ensure the code produces the same output.

Alternatively, in many cases, the person who collected the data may not
know how to code in a scripting language. However, he or she still has
incentive to make the data tidy for a statistician to speed the process
of collaboration. In this case, the statistician should be provided
something called pseudocode, which is simply a simple explanation, often
broken down into steps, to explain what has been done to the data
(\textbf{Figure \ref{fig1}D}). It should look something like:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Step 1 - take the raw file, run version 3.1.2 of summarize software
  with parameters a=1, b=2, c=3
\item
  Step 2 - run the software separately for each sample
\item
  Step 3 - take column three of outputfile.out for each sample and that
  is the corresponding row in the output data set
\end{enumerate}

You should also include information about which system
(Mac/Windows/Linux) you used the software on, the specific version of
any software used, and whether you tried it more than once to confirm it
gave the same results. Ideally, you will run this by a fellow student or
labmate to confirm that they can obtain the same output file you did.

\section{What you should expect from the
analyst}\label{what-you-should-expect-from-the-analyst}

When you turn over a properly tidied data set it dramatically decreases
the workload on the statistician and minimizes the likelihood of errors
during analysis. By taking the time to tidy the data, the data
generator, who knows the details of the data generated better than
anyone else, can expect to get the analysis back sooner and can be more
confident in its accuracy. Careful statisticians will check your recipe,
ask questions about steps you performed, and try to confirm that they
can obtain the same tidy data that you did with, at minimum, spot
checks.

You should then expect from the statistician:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An analysis script that performs each of the analyses (not just
  instructions).
\item
  The exact computer code they used to run the analysis.
\item
  All output files and figures they generated.
\end{enumerate}

This is the information you will use in the supplement to establish
reproducibility and precision of your results. Each of the steps in the
analysis should be clearly explained and you should ask questions when
you don't understand what the analyst did. It is the responsibility of
both the statistician and the scientist to understand the statistical
analysis. You may not be able to perform the exact analyses without the
statistician's code, but you should be able to explain why the
statistician performed each step to a labmate or your principal
investigator.

\section{Discussion}\label{discussion}

These guidelines aim to provide guidelines for effective and efficient
data sharing between those generating data and those analyzing it. We
highlight the need for data generators to (1) provide data in a tidy and
consistently coded format, (2) include all the necessary experimental
information regarding data generation, and (3) to explain any steps
taken to pre-process the data. If followed, these guidelines will both
speed up analysis turnaround time and minimize the likelihood of errors
during analysis.

\section{Funding}\label{funding}

This work was supported by NIH R01 GM105705.

\bibliographystyle{agsm}
\bibliography{DataSharing.bib}

\end{document}
